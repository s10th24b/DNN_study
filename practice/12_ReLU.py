# sigmoid보다 ReLU가 더 좋아
# 오늘은 정말 깊게 들어가는 방법에 대해 얘기해봄.

# 레이어 2개,3개인 XOR 구현하면서 시그모이드 썼다.

# 만약 히든 레이어를 9개 만들고 싶으면, 인풋, 아웃풋 포함해 총 W,b는 11개 필요하다.
# 그런데 막상 해보니, cost가 안떨어지고 정확도가 떨어진다.
# 결과가 2개 한것보다 안좋더라. 왜? Backpropagation 때문에. 1986년에 개발됨. 사람들이 흥분함.
# 근데, 2단 3단 정도의 네트워크는 잘 학습되는데, 9,10단 넘어가면서 학습이 안되더라. 큰 문제가 됨.
# 예전에 미분으로 chain rule 적용했던걸 기억해보자. 각 요소들은 거꾸로 차례대로 chain rule로 미분값들을 곱하게 된다.
# sigmoid 활성화 함수를 지나치면서 각각의 미분값들이 모두 0~1의 값이랑 곱해지게된다.
# 만약 0.01이었으면 한번만 곱해도 0.01과 이와 비슷한 값과 곱해지는거다. 항상 1보다 작은 값이고, 운이 안좋으면 거의 0에 수렴하게 된다.
# 그럼 이 값은 어떻게 되는가? 계속 1보다 작은 값을 곱하게 되어, 뒤로 갌록 곱해지는 항이 많아서 값은 훨씬 더 작아지게 된다.
# 이 말은, 맨 처음의 입력이 최종 결과에 영향을 미치지 않는다는 말이 된다. 이를 Vanishing Gradient라고 한다. (NN winter 2)
# 최종단 근처의 기울기는 나타나지만 단수가 깊어질수록 끝에는 경사도가 사라지게 된다.
# 이 문제때문에 neural network는 2차 겨울에 들어선다. 2006년까지.....
# 조프리 힌튼의 요약. 비선형 함수로 시그모이드를 쓰면 안됐어. 다른걸 썼어야 했는데...
# 시그모이드가 이런 문제를 불러일으켰지? 0~1의 값이니까 backpropagation할때 chain rule에서 점점 작아지니까
# 아주 간단하게 만든게 ReLU라는 Function이다. 아주 간단. x가 0보다 작으면 꺼버리고, 0보다 크면 갈때까지 간다.
# ReLU : Rectified Linear Unit. Activation 함수로 Sigmoid 를 넣는 대신에 이걸 넣자. 그래서 입력층 끝까지 가중치에 대한 오차가 잘 전파되더라.
# L1 = tf.sigmoid(tf.matmul(X,W1)+b1) # 신경망에서 sigmoid 쓰면 좋지않다.
# L1 = tf.nn.relu(tf.matmul(X,W1)+b1) #ReLU의 구현도 간단하다. f(x) = max(0,X) 하면 됨.
# 그런데 입력층과 히든레이어는 다 relu 쓰는데 마지막층은 시그모이드 쓴다.
# 마지막 단의 출력은 0~1이어야 한다.예제에서 우리가 0~1사이의 바이너리로 출력을 받기 때문.

# ReLU말고도 다른 함수들이 많다. ReLU에도 단점이 있었으니, 0보다 작은 입력값에대한 출력값이 항상 0이기에...
# 0에서 너무 확 끄니까 조금 설렁설렁하게 넣어주자.
